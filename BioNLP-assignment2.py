# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KLi_FUK8jdZeKJFN8YPk4wJ46coq0Aj0

SVM
"""

'''
CS584 BioNLP
Week 5 (THU)

@author: Abeed Sarker

'''

import nltk
nltk.download('stopwords')
import numpy as np

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import svm
from nltk.stem.porter import *
from nltk.corpus import stopwords
from sklearn.model_selection import StratifiedKFold
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV


stemmer = PorterStemmer()



def loadDataAsDataFrame(f_path):
    '''
        Given a path, loads a data set and puts it into a dataframe
        - simplified mechanism
    '''
    df = pd.read_csv(f_path)
    return df


def preprocess_text(raw_text):
    '''
        Preprocessing function
        PROGRAMMING TIP: Always a good idea to have a *master* preprocessing function that reads in a string and returns the
        preprocessed string after applying a series of functions.
    '''
    # stemming and lowercasing (no stopword removal
    words = [stemmer.stem(w) for w in raw_text.lower().split()]
    return (" ".join(words))

def grid_search_hyperparam_space(params, pipeline, folds, training_texts, training_classes):#folds, x_train, y_train, x_validation, y_validation):
        grid_search = GridSearchCV(estimator=pipeline, param_grid=params, refit=True, cv=folds, return_train_score=False, scoring='accuracy',n_jobs=-1)
        grid_search.fit(training_texts, training_classes)
        return grid_search


if __name__ == '__main__':
    # Load the data
    f_path = '/content/pdfalls.csv'
    data = loadDataAsDataFrame(f_path)
    texts =  data['fall_description']
    #print(texts)
    classes = data['fall_class']
    classes = classes.replace('BoS','Other')
    ids = data['record_id']
    location = data['fall_location']
    age = data['age']
    duration = data['duration']
    fall_study_day = data['fall_study_day']

    # SPLIT THE DATA (we could use sklearn.model_selection.train_test_split)
    training_set_size = int(0.8 * len(data))
    training_data = data[:training_set_size]
    training_texts = texts[:training_set_size]
    training_classes = classes[:training_set_size]
    training_ids = ids[:training_set_size]
    training_location = location[:training_set_size]
    training_age = age[:training_set_size]
    training_duration = duration[:training_set_size]
    training_days = fall_study_day[:training_set_size]

    test_data = data[training_set_size:]
    test_texts = texts[training_set_size:]
    test_classes = classes[training_set_size:]
    test_ids = ids[training_set_size:]
    test_location = location[training_set_size:]
    test_age = age[training_set_size:]
    test_duration = duration[training_set_size:]
    test_days = fall_study_day[training_set_size:]

    # PREPROCESS THE DATA
    training_texts_preprocessed = [preprocess_text(tr) for tr in training_texts]
    test_texts_preprocessed = [preprocess_text(te) for te in test_texts]
    training_location_preprocessed = [preprocess_text(tr) for tr in training_location]
    test_location_preprocessed = [preprocess_text(te) for te in test_location]

    vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer="word", tokenizer=None, preprocessor=None,
                                 max_features=5000)
    
    training_text_vectors = vectorizer.fit_transform(training_texts_preprocessed).toarray()
    test_text_vectors = vectorizer.transform(test_texts_preprocessed).toarray()

    training_loc_vectors = vectorizer.fit_transform(training_location_preprocessed).toarray()
    test_loc_vectors = vectorizer.transform(test_location_preprocessed).toarray()

    training_data_age = training_age.array
    test_data_age = test_age.array
    #print(training_data_age)
    training_age_vectors = []
    test_age_vectors = []
    for age in training_data_age:
      #print(age)
      training_age_vectors.append(age)
    training_age_vectors = np.array(training_age_vectors).reshape(-1,1)
    for age in test_data_age:
      #print(age)
      test_age_vectors.append(age)
    test_age_vectors = np.array(test_age_vectors).reshape(-1,1)


    training_data_duration = training_duration.array
    test_data_duration = test_duration.array
    #print(training_data_age)
    training_duration_vectors = []
    test_duration_vectors = []
    for duration in training_data_duration:
      #print(age)
      training_duration_vectors.append(duration)
    training_duration_vectors = np.array(training_duration_vectors).reshape(-1,1)
    for duration in test_data_duration:
      #print(age)
      test_duration_vectors.append(duration)
    test_duration_vectors = np.array(test_duration_vectors).reshape(-1,1)

    training_data_days = training_days.array
    test_data_days = test_days.array
    #print(training_data_age)
    training_days_vectors = []
    test_days_vectors = []
    for days in training_data_days:
      #print(age)
      training_days_vectors.append(days)
    training_days_vectors = np.array(training_days_vectors).reshape(-1,1)
    for days in test_data_duration:
      #print(age)
      test_days_vectors.append(days)
    test_days_vectors = np.array(test_days_vectors).reshape(-1,1)

    #training_data_vectors = training_text_vectors
    #test_data_vectors = test_text_vectors
    training_data_vectors = np.concatenate((training_age_vectors, training_text_vectors, training_days_vectors, training_duration_vectors, training_loc_vectors), axis=1)
    test_data_vectors = np.concatenate((test_age_vectors, test_text_vectors, test_days_vectors, test_duration_vectors, test_loc_vectors), axis=1)

    '''
        #PROGRAMMING TIP: c++ style coding here can help when doing feature engineering.. see below   
        training_texts_preprocessed = []
        for tr in training_texts:
            # you can do more with the training text here and generate more features...
            training_texts_preprocessed.append(preprocess_text(tr))
        '''

    
    #CLASSIFIER
    #gnb = GaussianNB()
    svm_classifier = svm.SVC(gamma='scale')

    #SIMPLE PIPELINE
    pipeline = Pipeline(steps = [('svm_classifier',svm_classifier)])
    #pipeline ensures vectorization happens in each fold of grid search (you could code the entire process manually for more flexibility)

    grid_params = {
         'svm_classifier__C': [1,2,4,8,16 ,32,64,128],
         'svm_classifier__kernel': ['linear','rbf'],
    }

    #SEARCH HYPERPARAMETERS
    folds = 10
    print(training_texts_preprocessed)
    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_data_vectors,training_classes)
    #grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_data_vectors,training_classes)

    print('Best hyperparameters:')
    print(grid.best_params_)

    print('All scores:')
    all_means = grid.cv_results_['mean_test_score']
    all_standard_devs = grid.cv_results_['std_test_score']
    all_params = grid.cv_results_['params']
    for mean, std, params in zip(all_means, all_standard_devs, all_params ):
        print('Mean:',mean, 'Standard deviation:', std, 'Hyperparameters:',  params)

    c_ = grid.best_params_['svm_classifier__C']
    kernel_ = grid.best_params_['svm_classifier__kernel']

    print('Optimal C found:',c_)
    print('Optimal kernel',kernel_)

    #CLASSIFY AND EVALUATE 
    #predictions_test = grid.predict(test_texts_preprocessed)
    predictions_test = grid.predict(test_data_vectors)
    print('Performance on held-out test set ... :')

    from sklearn.metrics import accuracy_score,f1_score
    print(accuracy_score(predictions_test,test_classes))
    print(f1_score(test_classes,predictions_test,average='micro'))
    print(f1_score(test_classes,predictions_test,average='macro'))



    """from sklearn.metrics import accuracy_score,f1_score

            print (accuracy_score(predictions, ttp_test))
            #print(f1_score(ttp_test,predictions,average='micro'))
            #print(f1_score(ttp_test,predictions,average='macro'))
            print('----')    # ke yi qiu average"""


        #print('----')

"""NB"""

'''
CS584 BioNLP
Week 5 (THU)

@author: Abeed Sarker

'''

import numpy as np
import nltk
nltk.download('stopwords')
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import svm
from nltk.stem.porter import *
from nltk.corpus import stopwords
from sklearn.model_selection import StratifiedKFold
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

st = stopwords.words('english')
stemmer = PorterStemmer()



def loadDataAsDataFrame(f_path):
    '''
        Given a path, loads a data set and puts it into a dataframe
        - simplified mechanism
    '''
    df = pd.read_csv(f_path)
    return df


def preprocess_text(raw_text):
    '''
        Preprocessing function
        PROGRAMMING TIP: Always a good idea to have a *master* preprocessing function that reads in a string and returns the
        preprocessed string after applying a series of functions.
    '''
    # stemming and lowercasing (no stopword removal
    words = [stemmer.stem(w) for w in raw_text.lower().split()]
    return (" ".join(words))


if __name__ == '__main__':
    # Load the data
    f_path = '/content/pdfalls.csv'
    data = loadDataAsDataFrame(f_path)
    texts = data['fall_description']
    classes = data['fall_class']
    classes = classes.replace('BoS','Other')
    ids = data['record_id']

    location = data['fall_location']
    age = data['age']
    duration = data['duration']
    fall_study_day = data['fall_study_day']

    # SPLIT THE DATA (we could use sklearn.model_selection.train_test_split)
    training_set_size = int(0.8*len(data))
    training_data = data[:training_set_size]
    training_texts = texts[:training_set_size]
    training_classes = classes[:training_set_size]
    training_ids = ids[:training_set_size]
    training_location = location[:training_set_size]
    training_age = age[:training_set_size]
    training_duration = duration[:training_set_size]
    training_days = fall_study_day[:training_set_size]

    test_data = data[training_set_size:]
    test_texts = texts[training_set_size:]
    test_classes = classes[training_set_size:]
    test_ids = ids[training_set_size:]
    test_location = location[training_set_size:]
    test_age = age[training_set_size:]
    test_duration = duration[training_set_size:]
    test_days = fall_study_day[training_set_size:]

    # PREPROCESS THE DATA
    training_texts_preprocessed = [preprocess_text(tr) for tr in training_texts]
    test_texts_preprocessed = [preprocess_text(te) for te in test_texts]
    
    training_location_preprocessed = [preprocess_text(tr) for tr in training_location]
    test_location_preprocessed = [preprocess_text(te) for te in test_location]

    vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer="word", tokenizer=None, preprocessor=None,
                                 max_features=5000)
    training_text_vectors = vectorizer.fit_transform(training_texts_preprocessed).toarray()
    test_text_vectors = vectorizer.transform(test_texts_preprocessed).toarray()

    training_loc_vectors = vectorizer.fit_transform(training_location_preprocessed).toarray()
    test_loc_vectors = vectorizer.transform(test_location_preprocessed).toarray()

    training_data_age = training_age.array
    test_data_age = test_age.array
    #print(training_data_age)
    training_age_vectors = []
    test_age_vectors = []
    for age in training_data_age:
      #print(age)
      training_age_vectors.append(age)
    training_age_vectors = np.array(training_age_vectors).reshape(-1,1)
    for age in test_data_age:
      #print(age)
      test_age_vectors.append(age)
    test_age_vectors = np.array(test_age_vectors).reshape(-1,1)


    training_data_duration = training_duration.array
    test_data_duration = test_duration.array
    #print(training_data_age)
    training_duration_vectors = []
    test_duration_vectors = []
    for duration in training_data_duration:
      #print(age)
      training_duration_vectors.append(duration)
    training_duration_vectors = np.array(training_duration_vectors).reshape(-1,1)
    for duration in test_data_duration:
      #print(age)
      test_duration_vectors.append(duration)
    test_duration_vectors = np.array(test_duration_vectors).reshape(-1,1)

    training_data_days = training_days.array
    test_data_days = test_days.array
    #print(training_data_age)
    training_days_vectors = []
    test_days_vectors = []
    for days in training_data_days:
      #print(age)
      training_days_vectors.append(days)
    training_days_vectors = np.array(training_days_vectors).reshape(-1,1)
    for days in test_data_duration:
      #print(age)
      test_days_vectors.append(days)
    test_days_vectors = np.array(test_days_vectors).reshape(-1,1)

    training_data_vectors = np.concatenate((training_age_vectors, training_text_vectors, training_days_vectors, training_duration_vectors, training_loc_vectors), axis=1)
    test_data_vectors = np.concatenate((test_age_vectors, test_text_vectors, test_days_vectors, test_duration_vectors, test_loc_vectors), axis=1)

    #training_data_vectors = training_loc_vectors
    #test_data_vectors = test_loc_vectors
                                       
    #print(test_duration_vectors)
    #training_data_vectors = training_text_vectors
    #test_data_vectors = test_text_vectors

    '''
        #PROGRAMMING TIP: c++ style coding here can help when doing feature engineering.. see below   
        training_texts_preprocessed = []
        for tr in training_texts:
            # you can do more with the training text here and generate more features...
            training_texts_preprocessed.append(preprocess_text(tr))
        '''

    
    #TRAIN THE MODEL
    gnb_classifier = GaussianNB()
    '''
    svm_classifier = svm.SVC(C=1, cache_size=200,
                             coef0=0.0, degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=True,
                             random_state=None, shrinking=True, tol=0.001, verbose=False)
    
'''
    gnb_classifier = gnb_classifier.fit(training_data_vectors, training_classes)
    #classifier = gnb_classifier.fit(training_data_vectors, training_classes)
    #svm_classifier = svm_classifier.fit(training_data_vectors, training_classes)
            

    # EVALUATE
    predictions = gnb_classifier.predict(test_data_vectors)
    #predictions = svm_classifier.predict(test_data_vectors)
    from sklearn.metrics import accuracy_score,f1_score
    print (accuracy_score(predictions,test_classes))
    print(f1_score(test_classes,predictions,average='micro'))
    print(f1_score(test_classes,predictions,average='macro'))

"""LOG"""

'''
CS584 BioNLP
Week 5 (THU)

@author: Abeed Sarker

'''

import nltk
nltk.download('stopwords')
import numpy as np

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import svm
from nltk.stem.porter import *
from nltk.corpus import stopwords
from sklearn.model_selection import StratifiedKFold
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression


stemmer = PorterStemmer()



def loadDataAsDataFrame(f_path):
    '''
        Given a path, loads a data set and puts it into a dataframe
        - simplified mechanism
    '''
    df = pd.read_csv(f_path)
    return df


def preprocess_text(raw_text):
    '''
        Preprocessing function
        PROGRAMMING TIP: Always a good idea to have a *master* preprocessing function that reads in a string and returns the
        preprocessed string after applying a series of functions.
    '''
    # stemming and lowercasing (no stopword removal
    words = [stemmer.stem(w) for w in raw_text.lower().split()]
    return (" ".join(words))

def grid_search_hyperparam_space(params, pipeline, folds, training_texts, training_classes):#folds, x_train, y_train, x_validation, y_validation):
        grid_search = GridSearchCV(estimator=pipeline, param_grid=params, refit=True, cv=folds, return_train_score=False, scoring='accuracy',n_jobs=-1)
        grid_search.fit(training_texts, training_classes)
        return grid_search


if __name__ == '__main__':
    # Load the data
    f_path = '/content/pdfalls.csv'
    data = loadDataAsDataFrame(f_path)
    texts =  data['fall_description']
    #print(texts)
    classes = data['fall_class']
    classes = classes.replace('BoS','Other')
    ids = data['record_id']
    location = data['fall_location']
    age = data['age']
    duration = data['duration']
    fall_study_day = data['fall_study_day']

    # SPLIT THE DATA (we could use sklearn.model_selection.train_test_split)
    training_set_size = int(0.8 * len(data))
    training_data = data[:training_set_size]
    training_texts = texts[:training_set_size]
    training_classes = classes[:training_set_size]
    training_ids = ids[:training_set_size]
    training_location = location[:training_set_size]
    training_age = age[:training_set_size]
    training_duration = duration[:training_set_size]
    training_days = fall_study_day[:training_set_size]

    test_data = data[training_set_size:]
    test_texts = texts[training_set_size:]
    test_classes = classes[training_set_size:]
    test_ids = ids[training_set_size:]
    test_location = location[training_set_size:]
    test_age = age[training_set_size:]
    test_duration = duration[training_set_size:]
    test_days = fall_study_day[training_set_size:]

    # PREPROCESS THE DATA
    training_texts_preprocessed = [preprocess_text(tr) for tr in training_texts]
    test_texts_preprocessed = [preprocess_text(te) for te in test_texts]
    training_location_preprocessed = [preprocess_text(tr) for tr in training_location]
    test_location_preprocessed = [preprocess_text(te) for te in test_location]

    vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer="word", tokenizer=None, preprocessor=None,
                                 max_features=5000)
    
    training_text_vectors = vectorizer.fit_transform(training_texts_preprocessed).toarray()
    test_text_vectors = vectorizer.transform(test_texts_preprocessed).toarray()

    training_loc_vectors = vectorizer.fit_transform(training_location_preprocessed).toarray()
    test_loc_vectors = vectorizer.transform(test_location_preprocessed).toarray()

    training_data_age = training_age.array
    test_data_age = test_age.array
    #print(training_data_age)
    training_age_vectors = []
    test_age_vectors = []
    for age in training_data_age:
      #print(age)
      training_age_vectors.append(age)
    training_age_vectors = np.array(training_age_vectors).reshape(-1,1)
    for age in test_data_age:
      #print(age)
      test_age_vectors.append(age)
    test_age_vectors = np.array(test_age_vectors).reshape(-1,1)


    training_data_duration = training_duration.array
    test_data_duration = test_duration.array
    #print(training_data_age)
    training_duration_vectors = []
    test_duration_vectors = []
    for duration in training_data_duration:
      #print(age)
      training_duration_vectors.append(duration)
    training_duration_vectors = np.array(training_duration_vectors).reshape(-1,1)
    for duration in test_data_duration:
      #print(age)
      test_duration_vectors.append(duration)
    test_duration_vectors = np.array(test_duration_vectors).reshape(-1,1)

    training_data_days = training_days.array
    test_data_days = test_days.array
    #print(training_data_age)
    training_days_vectors = []
    test_days_vectors = []
    for days in training_data_days:
      #print(age)
      training_days_vectors.append(days)
    training_days_vectors = np.array(training_days_vectors).reshape(-1,1)
    for days in test_data_duration:
      #print(age)
      test_days_vectors.append(days)
    test_days_vectors = np.array(test_days_vectors).reshape(-1,1)


    training_data_vectors = np.concatenate((training_age_vectors, training_text_vectors, training_days_vectors, training_duration_vectors, training_loc_vectors), axis=1)
    test_data_vectors = np.concatenate((test_age_vectors, test_text_vectors, test_days_vectors, test_duration_vectors, test_loc_vectors), axis=1)

    #training_data_vectors = training_text_vectors
    #test_data_vectors = test_text_vectors
    '''
        #PROGRAMMING TIP: c++ style coding here can help when doing feature engineering.. see below   
        training_texts_preprocessed = []
        for tr in training_texts:
            # you can do more with the training text here and generate more features...
            training_texts_preprocessed.append(preprocess_text(tr))
        '''

    
    #CLASSIFIER
    #gnb = GaussianNB()
    log_classifier = LogisticRegression(random_state=0)

    #SIMPLE PIPELINE
    pipeline = Pipeline(steps = [('log_classifier',log_classifier)])
    #pipeline ensures vectorization happens in each fold of grid search (you could code the entire process manually for more flexibility)

    grid_params = {
         'log_classifier__penalty': ['l2','l1','elasticnet'],
         'log_classifier__C': [1.0, 10.0, 100.0, 1000.0],
    }

    #SEARCH HYPERPARAMETERS
    folds = 5
    print(training_texts_preprocessed)
    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_data_vectors,training_classes)
    #grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_data_vectors,training_classes)

    print('Best hyperparameters:')
    print(grid.best_params_)

    print('All scores:')
    all_means = grid.cv_results_['mean_test_score']
    all_standard_devs = grid.cv_results_['std_test_score']
    all_params = grid.cv_results_['params']
    for mean, std, params in zip(all_means, all_standard_devs, all_params ):
        print('Mean:',mean, 'Standard deviation:', std, 'Hyperparameters:',  params)

    c_ = grid.best_params_['log_classifier__C']
    penalty_ = grid.best_params_['log_classifier__penalty']

    print('Optimal C found:',c_)
    print('Optimal penalty',penalty_)

    #CLASSIFY AND EVALUATE 
    #predictions_test = grid.predict(test_texts_preprocessed)
    predictions_test = grid.predict(test_data_vectors)
    print('Performance on held-out test set ... :')

    from sklearn.metrics import accuracy_score,f1_score
    print(accuracy_score(predictions_test,test_classes))
    print(f1_score(test_classes,predictions_test,average='micro'))
    print(f1_score(test_classes,predictions_test,average='macro'))



    """from sklearn.metrics import accuracy_score,f1_score

            print (accuracy_score(predictions, ttp_test))
            #print(f1_score(ttp_test,predictions,average='micro'))
            #print(f1_score(ttp_test,predictions,average='macro'))
            print('----')    # ke yi qiu average"""


        #print('----')

"""Decision Tree"""

'''
CS584 BioNLP
Week 5 (THU)

@author: Abeed Sarker

'''

import nltk
nltk.download('stopwords')
import numpy as np

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import svm
from nltk.stem.porter import *
from nltk.corpus import stopwords
from sklearn.model_selection import StratifiedKFold
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

stemmer = PorterStemmer()



def loadDataAsDataFrame(f_path):
    '''
        Given a path, loads a data set and puts it into a dataframe
        - simplified mechanism
    '''
    df = pd.read_csv(f_path)
    return df


def preprocess_text(raw_text):
    '''
        Preprocessing function
        PROGRAMMING TIP: Always a good idea to have a *master* preprocessing function that reads in a string and returns the
        preprocessed string after applying a series of functions.
    '''
    # stemming and lowercasing (no stopword removal
    words = [stemmer.stem(w) for w in raw_text.lower().split()]
    return (" ".join(words))

def grid_search_hyperparam_space(params, pipeline, folds, training_texts, training_classes):#folds, x_train, y_train, x_validation, y_validation):
        grid_search = GridSearchCV(estimator=pipeline, param_grid=params, refit=True, cv=folds, return_train_score=False, scoring='accuracy',n_jobs=-1)
        grid_search.fit(training_texts, training_classes)
        return grid_search


if __name__ == '__main__':
    # Load the data
    f_path = '/content/pdfalls.csv'
    data = loadDataAsDataFrame(f_path)
    texts =  data['fall_description']
    #print(texts)
    classes = data['fall_class']
    classes = classes.replace('BoS','Other')
    ids = data['record_id']
    location = data['fall_location']
    age = data['age']
    duration = data['duration']
    fall_study_day = data['fall_study_day']

    # SPLIT THE DATA (we could use sklearn.model_selection.train_test_split)
    training_set_size = int(0.7 * len(data))
    training_data = data[:training_set_size]
    training_texts = texts[:training_set_size]
    training_classes = classes[:training_set_size]
    training_ids = ids[:training_set_size]
    training_location = location[:training_set_size]
    training_age = age[:training_set_size]
    training_duration = duration[:training_set_size]
    training_days = fall_study_day[:training_set_size]

    test_data = data[training_set_size:]
    test_texts = texts[training_set_size:]
    test_classes = classes[training_set_size:]
    test_ids = ids[training_set_size:]
    test_location = location[training_set_size:]
    test_age = age[training_set_size:]
    test_duration = duration[training_set_size:]
    test_days = fall_study_day[training_set_size:]

    # PREPROCESS THE DATA
    training_texts_preprocessed = [preprocess_text(tr) for tr in training_texts]
    test_texts_preprocessed = [preprocess_text(te) for te in test_texts]
    training_location_preprocessed = [preprocess_text(tr) for tr in training_location]
    test_location_preprocessed = [preprocess_text(te) for te in test_location]
    
    training_text_vectors = vectorizer.fit_transform(training_texts_preprocessed).toarray()
    test_text_vectors = vectorizer.transform(test_texts_preprocessed).toarray()

    training_loc_vectors = vectorizer.fit_transform(training_location_preprocessed).toarray()
    test_loc_vectors = vectorizer.transform(test_location_preprocessed).toarray()

    training_data_age = training_age.array
    test_data_age = test_age.array
    #print(training_data_age)
    training_age_vectors = []
    test_age_vectors = []
    for age in training_data_age:
      #print(age)
      training_age_vectors.append(age)
    training_age_vectors = np.array(training_age_vectors).reshape(-1,1)
    for age in test_data_age:
      #print(age)
      test_age_vectors.append(age)
    test_age_vectors = np.array(test_age_vectors).reshape(-1,1)


    training_data_duration = training_duration.array
    test_data_duration = test_duration.array
    #print(training_data_age)
    training_duration_vectors = []
    test_duration_vectors = []
    for duration in training_data_duration:
      #print(age)
      training_duration_vectors.append(duration)
    training_duration_vectors = np.array(training_duration_vectors).reshape(-1,1)
    for duration in test_data_duration:
      #print(age)
      test_duration_vectors.append(duration)
    test_duration_vectors = np.array(test_duration_vectors).reshape(-1,1)

    training_data_days = training_days.array
    test_data_days = test_days.array
    #print(training_data_age)
    training_days_vectors = []
    test_days_vectors = []
    for days in training_data_days:
      #print(age)
      training_days_vectors.append(days)
    training_days_vectors = np.array(training_days_vectors).reshape(-1,1)
    for days in test_data_duration:
      #print(age)
      test_days_vectors.append(days)
    test_days_vectors = np.array(test_days_vectors).reshape(-1,1)


    #training_data_vectors = np.concatenate((training_age_vectors, training_text_vectors, training_days_vectors, training_duration_vectors, training_loc_vectors), axis=1)
    #test_data_vectors = np.concatenate((test_age_vectors, test_text_vectors, test_days_vectors, test_duration_vectors, test_loc_vectors), axis=1)

    training_data_vectors = training_text_vectors
    test_data_vectors = test_text_vectors
    '''
        #PROGRAMMING TIP: c++ style coding here can help when doing feature engineering.. see below   
        training_texts_preprocessed = []
        for tr in training_texts:
            # you can do more with the training text here and generate more features...
            training_texts_preprocessed.append(preprocess_text(tr))
        '''

    vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer="word", tokenizer=None, preprocessor=None,
                                 max_features=10000)
    #CLASSIFIER
    #gnb = GaussianNB()
    dt_classifier = DecisionTreeClassifier(random_state=0)

    #SIMPLE PIPELINE
    pipeline = Pipeline(steps = [('dt_classifier',dt_classifier)])
    #pipeline ensures vectorization happens in each fold of grid search (you could code the entire process manually for more flexibility)

    grid_params = {
         'dt_classifier__criterion': ['gini', 'entropy'],
         'dt_classifier__max_depth': [1, 2, 3],
    }

    #SEARCH HYPERPARAMETERS
    folds = 5
    print(training_texts_preprocessed)
    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_data_vectors,training_classes)
    #grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_data_vectors,training_classes)

    print('Best hyperparameters:')
    print(grid.best_params_)

    print('All scores:')
    all_means = grid.cv_results_['mean_test_score']
    all_standard_devs = grid.cv_results_['std_test_score']
    all_params = grid.cv_results_['params']
    for mean, std, params in zip(all_means, all_standard_devs, all_params ):
        print('Mean:',mean, 'Standard deviation:', std, 'Hyperparameters:',  params)

    criterion_ = grid.best_params_['dt_classifier__criterion']
    max_depth_ = grid.best_params_['dt_classifier__max_depth']

    print('Optimal C found:',criterion_)
    print('Optimal penalty',max_depth_)

    #CLASSIFY AND EVALUATE 
    #predictions_test = grid.predict(test_texts_preprocessed)
    predictions_test = grid.predict(test_data_vectors)
    print('Performance on held-out test set ... :')

    from sklearn.metrics import accuracy_score,f1_score
    print(accuracy_score(predictions_test,test_classes))
    print(f1_score(test_classes,predictions_test,average='micro'))
    print(f1_score(test_classes,predictions_test,average='macro'))



    """from sklearn.metrics import accuracy_score,f1_score

            print (accuracy_score(predictions, ttp_test))
            #print(f1_score(ttp_test,predictions,average='micro'))
            #print(f1_score(ttp_test,predictions,average='macro'))
            print('----')    # ke yi qiu average"""


        #print('----')

"""RF"""

'''
CS584 BioNLP
Week 5 (THU)

@author: Abeed Sarker

'''

import nltk
nltk.download('stopwords')
import numpy as np

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import svm
from nltk.stem.porter import *
from nltk.corpus import stopwords
from sklearn.model_selection import StratifiedKFold
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

stemmer = PorterStemmer()



def loadDataAsDataFrame(f_path):
    '''
        Given a path, loads a data set and puts it into a dataframe
        - simplified mechanism
    '''
    df = pd.read_csv(f_path)
    return df


def preprocess_text(raw_text):
    '''
        Preprocessing function
        PROGRAMMING TIP: Always a good idea to have a *master* preprocessing function that reads in a string and returns the
        preprocessed string after applying a series of functions.
    '''
    # stemming and lowercasing (no stopword removal
    words = [stemmer.stem(w) for w in raw_text.lower().split()]
    return (" ".join(words))

def grid_search_hyperparam_space(params, pipeline, folds, training_texts, training_classes):#folds, x_train, y_train, x_validation, y_validation):
        grid_search = GridSearchCV(estimator=pipeline, param_grid=params, refit=True, cv=folds, return_train_score=False, scoring='accuracy',n_jobs=-1)
        grid_search.fit(training_texts, training_classes)
        return grid_search


if __name__ == '__main__':
    # Load the data
    f_path = '/content/pdfalls.csv'
    data = loadDataAsDataFrame(f_path)
    texts =  data['fall_description']
    #print(texts)
    classes = data['fall_class']
    classes = classes.replace('BoS','Other')
    ids = data['record_id']
    location = data['fall_location']
    age = data['age']
    duration = data['duration']
    fall_study_day = data['fall_study_day']

    # SPLIT THE DATA (we could use sklearn.model_selection.train_test_split)
    training_set_size = int(0.8 * len(data))
    training_data = data[:training_set_size]
    training_texts = texts[:training_set_size]
    training_classes = classes[:training_set_size]
    training_ids = ids[:training_set_size]
    training_location = location[:training_set_size]
    training_age = age[:training_set_size]
    training_duration = duration[:training_set_size]
    training_days = fall_study_day[:training_set_size]

    test_data = data[training_set_size:]
    test_texts = texts[training_set_size:]
    test_classes = classes[training_set_size:]
    test_ids = ids[training_set_size:]
    test_location = location[training_set_size:]
    test_age = age[training_set_size:]
    test_duration = duration[training_set_size:]
    test_days = fall_study_day[training_set_size:]

    # PREPROCESS THE DATA
    training_texts_preprocessed = [preprocess_text(tr) for tr in training_texts]
    test_texts_preprocessed = [preprocess_text(te) for te in test_texts]
    training_location_preprocessed = [preprocess_text(tr) for tr in training_location]
    test_location_preprocessed = [preprocess_text(te) for te in test_location]

    vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer="word", tokenizer=None, preprocessor=None,
                                 max_features=5000)
    
    training_text_vectors = vectorizer.fit_transform(training_texts_preprocessed).toarray()
    test_text_vectors = vectorizer.transform(test_texts_preprocessed).toarray()

    training_loc_vectors = vectorizer.fit_transform(training_location_preprocessed).toarray()
    test_loc_vectors = vectorizer.transform(test_location_preprocessed).toarray()

    training_data_age = training_age.array
    test_data_age = test_age.array
    #print(training_data_age)
    training_age_vectors = []
    test_age_vectors = []
    for age in training_data_age:
      #print(age)
      training_age_vectors.append(age)
    training_age_vectors = np.array(training_age_vectors).reshape(-1,1)
    for age in test_data_age:
      #print(age)
      test_age_vectors.append(age)
    test_age_vectors = np.array(test_age_vectors).reshape(-1,1)


    training_data_duration = training_duration.array
    test_data_duration = test_duration.array
    #print(training_data_age)
    training_duration_vectors = []
    test_duration_vectors = []
    for duration in training_data_duration:
      #print(age)
      training_duration_vectors.append(duration)
    training_duration_vectors = np.array(training_duration_vectors).reshape(-1,1)
    for duration in test_data_duration:
      #print(age)
      test_duration_vectors.append(duration)
    test_duration_vectors = np.array(test_duration_vectors).reshape(-1,1)

    training_data_days = training_days.array
    test_data_days = test_days.array
    #print(training_data_age)
    training_days_vectors = []
    test_days_vectors = []
    for days in training_data_days:
      #print(age)
      training_days_vectors.append(days)
    training_days_vectors = np.array(training_days_vectors).reshape(-1,1)
    for days in test_data_duration:
      #print(age)
      test_days_vectors.append(days)
    test_days_vectors = np.array(test_days_vectors).reshape(-1,1)


    training_data_vectors = np.concatenate((training_age_vectors, training_text_vectors, training_days_vectors, training_duration_vectors, training_loc_vectors), axis=1)
    test_data_vectors = np.concatenate((test_age_vectors, test_text_vectors, test_days_vectors, test_duration_vectors, test_loc_vectors), axis=1)

    #training_data_vectors = training_text_vectors
    #test_data_vectors = test_text_vectors
    '''
        #PROGRAMMING TIP: c++ style coding here can help when doing feature engineering.. see below   
        training_texts_preprocessed = []
        for tr in training_texts:
            # you can do more with the training text here and generate more features...
            training_texts_preprocessed.append(preprocess_text(tr))
        '''

   
    #CLASSIFIER
    #gnb = GaussianNB()
    rf_classifier = RandomForestClassifier(random_state=1)

    #SIMPLE PIPELINE
    pipeline = Pipeline(steps = [('rf_classifier',rf_classifier)])
    #pipeline ensures vectorization happens in each fold of grid search (you could code the entire process manually for more flexibility)

    grid_params = {
         'rf_classifier__criterion': ['gini', 'entropy'],
         'rf_classifier__n_estimators': [20,50,80,100],
    }

    #SEARCH HYPERPARAMETERS
    folds = 5
    print(training_texts_preprocessed)
    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_data_vectors,training_classes)
    #grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_data_vectors,training_classes)

    print('Best hyperparameters:')
    print(grid.best_params_)

    print('All scores:')
    all_means = grid.cv_results_['mean_test_score']
    all_standard_devs = grid.cv_results_['std_test_score']
    all_params = grid.cv_results_['params']
    for mean, std, params in zip(all_means, all_standard_devs, all_params ):
        print('Mean:',mean, 'Standard deviation:', std, 'Hyperparameters:',  params)

    criterion_ = grid.best_params_['rf_classifier__criterion']
    n_estimators_ = grid.best_params_['rf_classifier__n_estimators']

    print('Optimal C found:',criterion_)
    print('Optimal penalty',n_estimators_)

    #CLASSIFY AND EVALUATE 
    #predictions_test = grid.predict(test_texts_preprocessed)
    predictions_test = grid.predict(test_data_vectors)
    print('Performance on held-out test set ... :')

    from sklearn.metrics import accuracy_score,f1_score
    print(accuracy_score(predictions_test,test_classes))
    print(f1_score(test_classes,predictions_test,average='micro'))
    print(f1_score(test_classes,predictions_test,average='macro'))



    """from sklearn.metrics import accuracy_score,f1_score

            print (accuracy_score(predictions, ttp_test))
            #print(f1_score(ttp_test,predictions,average='micro'))
            #print(f1_score(ttp_test,predictions,average='macro'))
            print('----')    # ke yi qiu average"""


        #print('----')

"""KNN"""

'''
CS584 BioNLP
Week 5 (THU)

@author: Abeed Sarker

'''

import nltk
nltk.download('stopwords')
import numpy as np

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import svm
from nltk.stem.porter import *
from nltk.corpus import stopwords
from sklearn.model_selection import StratifiedKFold
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

stemmer = PorterStemmer()



def loadDataAsDataFrame(f_path):
    '''
        Given a path, loads a data set and puts it into a dataframe
        - simplified mechanism
    '''
    df = pd.read_csv(f_path)
    return df


def preprocess_text(raw_text):
    '''
        Preprocessing function
        PROGRAMMING TIP: Always a good idea to have a *master* preprocessing function that reads in a string and returns the
        preprocessed string after applying a series of functions.
    '''
    # stemming and lowercasing (no stopword removal
    words = [stemmer.stem(w) for w in raw_text.lower().split()]
    return (" ".join(words))

def grid_search_hyperparam_space(params, pipeline, folds, training_texts, training_classes):#folds, x_train, y_train, x_validation, y_validation):
        grid_search = GridSearchCV(estimator=pipeline, param_grid=params, refit=True, cv=folds, return_train_score=False, scoring='accuracy',n_jobs=-1)
        grid_search.fit(training_texts, training_classes)
        return grid_search


if __name__ == '__main__':
    # Load the data
    f_path = '/content/pdfalls.csv'
    data = loadDataAsDataFrame(f_path)
    texts =  data['fall_description']
    #print(texts)
    classes = data['fall_class']
    classes = classes.replace('BoS','Other')
    ids = data['record_id']
    location = data['fall_location']
    age = data['age']
    duration = data['duration']
    fall_study_day = data['fall_study_day']

    # SPLIT THE DATA (we could use sklearn.model_selection.train_test_split)
    training_set_size = int(0.8 * len(data))
    training_data = data[:training_set_size]
    training_texts = texts[:training_set_size]
    training_classes = classes[:training_set_size]
    training_ids = ids[:training_set_size]
    training_location = location[:training_set_size]
    training_age = age[:training_set_size]
    training_duration = duration[:training_set_size]
    training_days = fall_study_day[:training_set_size]

    test_data = data[training_set_size:]
    test_texts = texts[training_set_size:]
    test_classes = classes[training_set_size:]
    test_ids = ids[training_set_size:]
    test_location = location[training_set_size:]
    test_age = age[training_set_size:]
    test_duration = duration[training_set_size:]
    test_days = fall_study_day[training_set_size:]

    # PREPROCESS THE DATA
    training_texts_preprocessed = [preprocess_text(tr) for tr in training_texts]
    test_texts_preprocessed = [preprocess_text(te) for te in test_texts]
    training_location_preprocessed = [preprocess_text(tr) for tr in training_location]
    test_location_preprocessed = [preprocess_text(te) for te in test_location]
    
    training_text_vectors = vectorizer.fit_transform(training_texts_preprocessed).toarray()
    test_text_vectors = vectorizer.transform(test_texts_preprocessed).toarray()

    training_loc_vectors = vectorizer.fit_transform(training_location_preprocessed).toarray()
    test_loc_vectors = vectorizer.transform(test_location_preprocessed).toarray()

    training_data_age = training_age.array
    test_data_age = test_age.array
    #print(training_data_age)
    training_age_vectors = []
    test_age_vectors = []
    for age in training_data_age:
      #print(age)
      training_age_vectors.append(age)
    training_age_vectors = np.array(training_age_vectors).reshape(-1,1)
    for age in test_data_age:
      #print(age)
      test_age_vectors.append(age)
    test_age_vectors = np.array(test_age_vectors).reshape(-1,1)


    training_data_duration = training_duration.array
    test_data_duration = test_duration.array
    #print(training_data_age)
    training_duration_vectors = []
    test_duration_vectors = []
    for duration in training_data_duration:
      #print(age)
      training_duration_vectors.append(duration)
    training_duration_vectors = np.array(training_duration_vectors).reshape(-1,1)
    for duration in test_data_duration:
      #print(age)
      test_duration_vectors.append(duration)
    test_duration_vectors = np.array(test_duration_vectors).reshape(-1,1)

    training_data_days = training_days.array
    test_data_days = test_days.array
    #print(training_data_age)
    training_days_vectors = []
    test_days_vectors = []
    for days in training_data_days:
      #print(age)
      training_days_vectors.append(days)
    training_days_vectors = np.array(training_days_vectors).reshape(-1,1)
    for days in test_data_duration:
      #print(age)
      test_days_vectors.append(days)
    test_days_vectors = np.array(test_days_vectors).reshape(-1,1)


    training_data_vectors = np.concatenate((training_age_vectors, training_text_vectors, training_days_vectors, training_duration_vectors, training_loc_vectors), axis=1)
    test_data_vectors = np.concatenate((test_age_vectors, test_text_vectors, test_days_vectors, test_duration_vectors, test_loc_vectors), axis=1)

    #training_data_vectors = training_text_vectors
    #test_data_vectors = test_text_vectors
    '''
        #PROGRAMMING TIP: c++ style coding here can help when doing feature engineering.. see below   
        training_texts_preprocessed = []
        for tr in training_texts:
            # you can do more with the training text here and generate more features...
            training_texts_preprocessed.append(preprocess_text(tr))
        '''

    vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer="word", tokenizer=None, preprocessor=None,
                                 max_features=10000)
    #CLASSIFIER
    #gnb = GaussianNB()
    knn_classifier = KNeighborsClassifier()

    #SIMPLE PIPELINE
    pipeline = Pipeline(steps = [('knn_classifier',knn_classifier)])
    #pipeline ensures vectorization happens in each fold of grid search (you could code the entire process manually for more flexibility)

    grid_params = {
         'knn_classifier__p': [1,2],
         'knn_classifier__n_neighbors': [2,3,4,5],
         'knn_classifier__algorithm': ['auto', 'ball_tree', 'kd_tree' , 'brute'],
    }

    #SEARCH HYPERPARAMETERS
    folds = 5
    print(training_texts_preprocessed)
    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_data_vectors,training_classes)
    #grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_data_vectors,training_classes)

    print('Best hyperparameters:')
    print(grid.best_params_)

    print('All scores:')
    all_means = grid.cv_results_['mean_test_score']
    all_standard_devs = grid.cv_results_['std_test_score']
    all_params = grid.cv_results_['params']
    for mean, std, params in zip(all_means, all_standard_devs, all_params ):
        print('Mean:',mean, 'Standard deviation:', std, 'Hyperparameters:',  params)

    p_ = grid.best_params_['knn_classifier__p']
    n_neighbors_ = grid.best_params_['knn_classifier__n_neighbors']
    algorithm_ = grid.best_params_['knn_classifier__algorithm']

    print('Optimal p found:',p_)
    print('Optimal neighbors',n_neighbors_)
    print('Optimal algorithm:',algorithm_)

    #CLASSIFY AND EVALUATE 
    #predictions_test = grid.predict(test_texts_preprocessed)
    predictions_test = grid.predict(test_data_vectors)
    print('Performance on held-out test set ... :')

    from sklearn.metrics import accuracy_score,f1_score
    print(accuracy_score(predictions_test,test_classes))
    print(f1_score(test_classes,predictions_test,average='micro'))
    print(f1_score(test_classes,predictions_test,average='macro'))



    """from sklearn.metrics import accuracy_score,f1_score

            print (accuracy_score(predictions, ttp_test))
            #print(f1_score(ttp_test,predictions,average='micro'))
            #print(f1_score(ttp_test,predictions,average='macro'))
            print('----')    # ke yi qiu average"""


        #print('----')

"""ensemble"""

'''
CS584 BioNLP
Week 5 (THU)

@author: Abeed Sarker

'''

import nltk
nltk.download('stopwords')
import numpy as np

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import svm
from nltk.stem.porter import *
from nltk.corpus import stopwords
from sklearn.model_selection import StratifiedKFold
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score,f1_score

stemmer = PorterStemmer()



def loadDataAsDataFrame(f_path):
    '''
        Given a path, loads a data set and puts it into a dataframe
        - simplified mechanism
    '''
    df = pd.read_csv(f_path)
    return df


def preprocess_text(raw_text):
    '''
        Preprocessing function
        PROGRAMMING TIP: Always a good idea to have a *master* preprocessing function that reads in a string and returns the
        preprocessed string after applying a series of functions.
    '''
    # stemming and lowercasing (no stopword removal
    words = [stemmer.stem(w) for w in raw_text.lower().split()]
    return (" ".join(words))

def grid_search_hyperparam_space(params, pipeline, folds, training_texts, training_classes):#folds, x_train, y_train, x_validation, y_validation):
        grid_search = GridSearchCV(estimator=pipeline, param_grid=params, refit=True, cv=folds, return_train_score=False, scoring='accuracy',n_jobs=-1)
        grid_search.fit(training_texts, training_classes)
        return grid_search


if __name__ == '__main__':
    # Load the data
    f_path = '/content/pdfalls.csv'
    data = loadDataAsDataFrame(f_path)
    texts =  data['fall_description']
    #print(texts)
    classes = data['fall_class']
    classes = classes.replace('BoS','Other')
    ids = data['record_id']
    location = data['fall_location']
    age = data['age']
    duration = data['duration']
    fall_study_day = data['fall_study_day']

    # SPLIT THE DATA (we could use sklearn.model_selection.train_test_split)
    training_set_size = int(0.8 * len(data))
    training_data = data[:training_set_size]
    training_texts = texts[:training_set_size]
    training_classes = classes[:training_set_size]
    training_ids = ids[:training_set_size]
    training_location = location[:training_set_size]
    training_age = age[:training_set_size]
    training_duration = duration[:training_set_size]
    training_days = fall_study_day[:training_set_size]

    test_data = data[training_set_size:]
    test_texts = texts[training_set_size:]
    test_classes = classes[training_set_size:]
    test_ids = ids[training_set_size:]
    test_location = location[training_set_size:]
    test_age = age[training_set_size:]
    test_duration = duration[training_set_size:]
    test_days = fall_study_day[training_set_size:]

    # PREPROCESS THE DATA
    training_texts_preprocessed = [preprocess_text(tr) for tr in training_texts]
    test_texts_preprocessed = [preprocess_text(te) for te in test_texts]
    training_location_preprocessed = [preprocess_text(tr) for tr in training_location]
    test_location_preprocessed = [preprocess_text(te) for te in test_location]
    
    training_text_vectors = vectorizer.fit_transform(training_texts_preprocessed).toarray()
    test_text_vectors = vectorizer.transform(test_texts_preprocessed).toarray()

    training_loc_vectors = vectorizer.fit_transform(training_location_preprocessed).toarray()
    test_loc_vectors = vectorizer.transform(test_location_preprocessed).toarray()

    training_data_age = training_age.array
    test_data_age = test_age.array
    #print(training_data_age)
    training_age_vectors = []
    test_age_vectors = []
    for age in training_data_age:
      #print(age)
      training_age_vectors.append(age)
    training_age_vectors = np.array(training_age_vectors).reshape(-1,1)
    for age in test_data_age:
      #print(age)
      test_age_vectors.append(age)
    test_age_vectors = np.array(test_age_vectors).reshape(-1,1)


    training_data_duration = training_duration.array
    test_data_duration = test_duration.array
    #print(training_data_age)
    training_duration_vectors = []
    test_duration_vectors = []
    for duration in training_data_duration:
      #print(age)
      training_duration_vectors.append(duration)
    training_duration_vectors = np.array(training_duration_vectors).reshape(-1,1)
    for duration in test_data_duration:
      #print(age)
      test_duration_vectors.append(duration)
    test_duration_vectors = np.array(test_duration_vectors).reshape(-1,1)

    training_data_days = training_days.array
    test_data_days = test_days.array
    #print(training_data_age)
    training_days_vectors = []
    test_days_vectors = []
    for days in training_data_days:
      #print(age)
      training_days_vectors.append(days)
    training_days_vectors = np.array(training_days_vectors).reshape(-1,1)
    for days in test_data_duration:
      #print(age)
      test_days_vectors.append(days)
    test_days_vectors = np.array(test_days_vectors).reshape(-1,1)


    training_data_vectors = np.concatenate((training_age_vectors, training_days_vectors, training_duration_vectors, training_loc_vectors), axis=1)
    test_data_vectors = np.concatenate((test_age_vectors, test_days_vectors, test_duration_vectors, test_loc_vectors), axis=1)

    #training_data_vectors = training_text_vectors
    #test_data_vectors = test_text_vectors


clf1 = svm.SVC(gamma='auto',kernel='rbf')
clf2 = RandomForestClassifier(n_estimators=20, random_state=1,criterion='entropy')
clf3 = GaussianNB()
clf4 = KNeighborsClassifier(n_neighbors=2, p=2, algorithm='auto')
clf5 = DecisionTreeClassifier(random_state=0,max_depth=2)
clf6 = LogisticRegression(random_state=0)

eclf = VotingClassifier(estimators=[('svm', clf1), ('rf', clf2), ('gnb', clf3),('knn', clf4),('dt', clf5),('log', clf6)],voting='hard')

for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, eclf], ['SVM', 'Random Forest', 'naive Bayes', 'KNN', 'Decision Tree', 'Logistic Regression', 'Ensemble']):
  scores = cross_val_score(clf, training_data_vectors, training_classes, scoring='f1_macro', cv=5)
  print("f1_score: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))